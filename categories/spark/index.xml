<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Spark on Kirill Pavlov</title>
    <link>http://kirillpavlov.com/categories/spark/</link>
    <description>Recent content in Spark on Kirill Pavlov</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 23 Apr 2016 09:40:05 +0800</lastBuildDate>
    <atom:link href="http://kirillpavlov.com/categories/spark/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Beyond traditional join with Apache Spark</title>
      <link>http://kirillpavlov.com/blog/2016/04/23/beyond-traditional-join-with-apache-spark/</link>
      <pubDate>Sat, 23 Apr 2016 09:40:05 +0800</pubDate>
      
      <guid>http://kirillpavlov.com/blog/2016/04/23/beyond-traditional-join-with-apache-spark/</guid>
      <description>

&lt;p&gt;An &lt;a href=&#34;https://en.wikipedia.org/wiki/Join_(SQL)&#34;&gt;SQL join&lt;/a&gt; clause combines records from two or more tables.
This operation is very common in data processing and understanding of what happens under the hood is important.
There are several common join types: &lt;code&gt;INNER&lt;/code&gt;, &lt;code&gt;LEFT OUTER&lt;/code&gt;, &lt;code&gt;RIGHT OUTER&lt;/code&gt;, &lt;code&gt;FULL OUTER&lt;/code&gt; and &lt;code&gt;CROSS&lt;/code&gt; or &lt;code&gt;CARTESIAN&lt;/code&gt;.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://kirillpavlov.com/images/join-types.png&#34; alt=&#34;join types&#34; /&gt;
    
    
&lt;/figure&gt;



&lt;p&gt;Join which uses the same table is a self-join.
If an operation uses equality operator, it is &lt;a href=&#34;https://en.wikipedia.org/wiki/Join_(SQL)#Equi-join&#34;&gt;equi-join&lt;/a&gt;, otherwise, it is non-equi-join.&lt;/p&gt;

&lt;p&gt;This article covers different join types in Apache Spark as well as examples of &lt;a href=&#34;https://en.wikipedia.org/wiki/Slowly_changing_dimension&#34;&gt;slowly changed dimensions (SCD)&lt;/a&gt; and joins on non-unique columns.&lt;/p&gt;

&lt;h3 id=&#34;sample-data:58684124e2b27686159fe7ccddda41aa&#34;&gt;Sample data&lt;/h3&gt;

&lt;p&gt;All subsequent explanations on join types in this article make use of the following two tables, taken from Wikipedia article.
The rows in these tables serve to illustrate the effect of different types of joins and join-predicates.&lt;/p&gt;

&lt;p&gt;Employees table has a nullable column. To express it in terms of statically typed Scala, one needs to use &lt;a href=&#34;http://www.scala-lang.org/api/current/#scala.Option&#34;&gt;Option&lt;/a&gt; type.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val employees = sc.parallelize(Array[(String, Option[Int])](
  (&amp;quot;Rafferty&amp;quot;, Some(31)), (&amp;quot;Jones&amp;quot;, Some(33)), (&amp;quot;Heisenberg&amp;quot;, Some(33)), (&amp;quot;Robinson&amp;quot;, Some(34)), (&amp;quot;Smith&amp;quot;, Some(34)), (&amp;quot;Williams&amp;quot;, null)
)).toDF(&amp;quot;LastName&amp;quot;, &amp;quot;DepartmentID&amp;quot;)

employees.show()

+----------+------------+
|  LastName|DepartmentID|
+----------+------------+
|  Rafferty|          31|
|     Jones|          33|
|Heisenberg|          33|
|  Robinson|          34|
|     Smith|          34|
|  Williams|        null|
+----------+------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Department table does not have nullable columns, type specification could be omitted.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val departments = sc.parallelize(Array(
  (31, &amp;quot;Sales&amp;quot;), (33, &amp;quot;Engineering&amp;quot;), (34, &amp;quot;Clerical&amp;quot;),
  (35, &amp;quot;Marketing&amp;quot;)
)).toDF(&amp;quot;DepartmentID&amp;quot;, &amp;quot;DepartmentName&amp;quot;)

departments.show()

+------------+--------------+
|DepartmentID|DepartmentName|
+------------+--------------+
|          31|         Sales|
|          33|   Engineering|
|          34|      Clerical|
|          35|     Marketing|
+------------+--------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;inner-join:58684124e2b27686159fe7ccddda41aa&#34;&gt;Inner join&lt;/h3&gt;

&lt;p&gt;Following SQL code&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;SELECT *
FROM employee 
INNER JOIN department
ON employee.DepartmentID = department.DepartmentID;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;could be written in Spark as&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;employees
  .join(departments, &amp;quot;DepartmentID&amp;quot;)
  .show()

+------------+----------+--------------+
|DepartmentID|  LastName|DepartmentName|
+------------+----------+--------------+
|          31|  Rafferty|         Sales|
|          33|     Jones|   Engineering|
|          33|Heisenberg|   Engineering|
|          34|  Robinson|      Clerical|
|          34|     Smith|      Clerical|
+------------+----------+--------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Beautiful, is not it? Spark automatically removes duplicated &amp;ldquo;DepartmentID&amp;rdquo; column, so column names are unique and one does not need to use table prefix to address them.&lt;/p&gt;

&lt;h3 id=&#34;left-outer-join:58684124e2b27686159fe7ccddda41aa&#34;&gt;Left outer join&lt;/h3&gt;

&lt;p&gt;Left outer join is a very common operation, especially if there are nulls or gaps in a data.
Note, that column name should be wrapped into scala &lt;code&gt;Seq&lt;/code&gt; if join type is specified.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;employees
  .join(departments, Seq(&amp;quot;DepartmentID&amp;quot;), &amp;quot;left_outer&amp;quot;)
  .show()

+------------+----------+--------------+
|DepartmentID|  LastName|DepartmentName|
+------------+----------+--------------+
|          31|  Rafferty|         Sales|
|          33|     Jones|   Engineering|
|          33|Heisenberg|   Engineering|
|          34|  Robinson|      Clerical|
|          34|     Smith|      Clerical|
|        null|  Williams|          null|
+------------+----------+--------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;other-join-types:58684124e2b27686159fe7ccddda41aa&#34;&gt;Other join types&lt;/h3&gt;

&lt;p&gt;Spark allows using following join types: &lt;code&gt;inner&lt;/code&gt;, &lt;code&gt;outer&lt;/code&gt;, &lt;code&gt;left_outer&lt;/code&gt;, &lt;code&gt;right_outer&lt;/code&gt;, &lt;code&gt;leftsemi&lt;/code&gt;.
The interface is the same as for left outer join in the example above.&lt;/p&gt;

&lt;p&gt;For cartesian join column specification should be omitted:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;employees
  .join(departments)
  .show(10)

+----------+------------+------------+--------------+
|  LastName|DepartmentID|DepartmentID|DepartmentName|
+----------+------------+------------+--------------+
|  Rafferty|          31|          31|         Sales|
|  Rafferty|          31|          33|   Engineering|
|  Rafferty|          31|          34|      Clerical|
|  Rafferty|          31|          35|     Marketing|
|     Jones|          33|          31|         Sales|
|     Jones|          33|          33|   Engineering|
|     Jones|          33|          34|      Clerical|
|     Jones|          33|          35|     Marketing|
|Heisenberg|          33|          31|         Sales|
|Heisenberg|          33|          33|   Engineering|
+----------+------------+------------+--------------+
only showing top 10 rows
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Warning: do not use cartesian join with big tables in production.&lt;/p&gt;

&lt;h3 id=&#34;join-expression-slowly-changing-dimensions-and-non-equi-join:58684124e2b27686159fe7ccddda41aa&#34;&gt;Join expression, slowly changing dimensions and non-equi join&lt;/h3&gt;

&lt;p&gt;Spark allows us to specify join expression instead of a sequence of columns.
In general, expression specification is less readable, so why do we need such flexibility?
The reason is non-equi join.&lt;/p&gt;

&lt;p&gt;One application of it is slowly changing dimensions.
Assume there is a table with product prices over time:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val products = sc.parallelize(Array(
  (&amp;quot;steak&amp;quot;, &amp;quot;1990-01-01&amp;quot;, &amp;quot;2000-01-01&amp;quot;, 150),
  (&amp;quot;steak&amp;quot;, &amp;quot;2000-01-02&amp;quot;, &amp;quot;2020-01-01&amp;quot;, 180),
  (&amp;quot;fish&amp;quot;, &amp;quot;1990-01-01&amp;quot;, &amp;quot;2020-01-01&amp;quot;, 100)
)).toDF(&amp;quot;name&amp;quot;, &amp;quot;startDate&amp;quot;, &amp;quot;endDate&amp;quot;, &amp;quot;price&amp;quot;)

products.show()

+-----+----------+----------+-----+
| name| startDate|   endDate|price|
+-----+----------+----------+-----+
|steak|1990-01-01|2000-01-01|  150|
|steak|2000-01-02|2020-01-01|  180|
| fish|1990-01-01|2020-01-01|  100|
+-----+----------+----------+-----+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are two products only: steak and fish, price of steak has been changed once.
Another table consists of product orders by day:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val orders = sc.parallelize(Array(
  (&amp;quot;1995-01-01&amp;quot;, &amp;quot;steak&amp;quot;),
  (&amp;quot;2000-01-01&amp;quot;, &amp;quot;fish&amp;quot;),
  (&amp;quot;2005-01-01&amp;quot;, &amp;quot;steak&amp;quot;)
)).toDF(&amp;quot;date&amp;quot;, &amp;quot;product&amp;quot;)

orders.show()

+----------+-------+
|      date|product|
+----------+-------+
|1995-01-01|  steak|
|2000-01-01|   fish|
|2005-01-01|  steak|
+----------+-------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our goal is to assign an actual price for every record in the orders table.
It is not obvious to do using only equality operators, however, spark join expression allows us to achieve the result in an elegant way:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;orders
  .join(products, $&amp;quot;product&amp;quot; === $&amp;quot;name&amp;quot; &amp;amp;&amp;amp; $&amp;quot;date&amp;quot; &amp;gt;= $&amp;quot;startDate&amp;quot; &amp;amp;&amp;amp; $&amp;quot;date&amp;quot; &amp;lt;= $&amp;quot;endDate&amp;quot;)
  .show()

+----------+-------+-----+----------+----------+-----+
|      date|product| name| startDate|   endDate|price|
+----------+-------+-----+----------+----------+-----+
|2000-01-01|   fish| fish|1990-01-01|2020-01-01|  100|
|1995-01-01|  steak|steak|1990-01-01|2000-01-01|  150|
|2005-01-01|  steak|steak|2000-01-02|2020-01-01|  180|
+----------+-------+-----+----------+----------+-----+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This technique is very useful, yet not that common.
It could save a lot of time for those who write as well as for those who read the code.&lt;/p&gt;

&lt;h3 id=&#34;inner-join-using-non-primary-keys:58684124e2b27686159fe7ccddda41aa&#34;&gt;Inner join using non primary keys&lt;/h3&gt;

&lt;p&gt;Last part of this article is about joins on non unique columns and common mistakes related to it.
Join (intersection) diagrams in the beginning of this article stuck in our heads.
Because of visual comparison of sets intersection we assume, that result table after inner join should be smaller, than any of the source tables.
This is correct only for joins on unique columns and wrong if columns in both tables are not unique.
Consider following DataFrame with duplicated records and its self-join:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val df = sc.parallelize(Array(
  (0), (1), (1)
)).toDF(&amp;quot;c1&amp;quot;)

df.show()
df.join(df, &amp;quot;c1&amp;quot;).show()

// Original DataFrame
+---+
| c1|
+---+
|  0|
|  1|
|  1|
+---+

// Self-joined DataFrame
+---+
| c1|
+---+
|  0|
|  1|
|  1|
|  1|
|  1|
+---+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note, that size of the result DataFrame is bigger than the source size. It could be as big as n&lt;sup&gt;2&lt;/sup&gt;, where n is a size of source.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:58684124e2b27686159fe7ccddda41aa&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;The article covered different join types implementations with Apache Spark, including join expressions and join on non-unique keys.&lt;/p&gt;

&lt;p&gt;Apache Spark allows developers to write the code in the way, which is easier to understand. It improves code quality and maintainability.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Top 5 features released in spark 1.6</title>
      <link>http://kirillpavlov.com/blog/2016/02/21/top-5-features-released-in-spark-1.6/</link>
      <pubDate>Sun, 21 Feb 2016 00:20:45 +0800</pubDate>
      
      <guid>http://kirillpavlov.com/blog/2016/02/21/top-5-features-released-in-spark-1.6/</guid>
      <description>

&lt;p&gt;Spark version 1.6 &lt;a href=&#34;https://spark.apache.org/releases/spark-release-1-6-0.html&#34;&gt;has been released&lt;/a&gt; on January 4th, 2016.
Compared to the previous version, it has significant improvements. This article covers top 5 of them.&lt;/p&gt;

&lt;h3 id=&#34;1-partition-by-column:8ab4cd76c2e48d03608ec5aca1374ea6&#34;&gt;1. Partition by column&lt;/h3&gt;

&lt;p&gt;The idea is to have more control on RDD&amp;rsquo;s partitioning.
Sometimes data needs to be joined and grouped by certain key, such as user_id.
To minify data reshuffling, one may possible to store chunks of objects with the same key within the same data node.&lt;/p&gt;

&lt;p&gt;This feature &lt;a href=&#34;https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SortBy#LanguageManualSortBy-SyntaxofClusterByandDistributeBy&#34;&gt;exists in Hive&lt;/a&gt; and has been &lt;a href=&#34;https://issues.apache.org/jira/browse/SPARK-11410&#34;&gt;ported to spark&lt;/a&gt;.
Example of usage:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val df = sc.parallelize(Array(
    (&amp;quot;A&amp;quot;, 1), (&amp;quot;B&amp;quot;, 2), (&amp;quot;C&amp;quot;, 3), (&amp;quot;A&amp;quot;, 4)
)).toDF(&amp;quot;key&amp;quot;, &amp;quot;value&amp;quot;)
val partitioned = df.repartition($&amp;quot;key&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-groupeddata-pivot:8ab4cd76c2e48d03608ec5aca1374ea6&#34;&gt;2. GroupedData Pivot&lt;/h3&gt;

&lt;p&gt;This feature is about data presentation: if we need to transform adjacency list to adjacency matrix or convert long narrow RDD to the matrix - pivot is our friend.
Python has pivot functionality in Pandas DataFrames: &lt;a href=&#34;http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.unstack.html#pandas.DataFrame.unstack&#34;&gt;unstack&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val df = sc.parallelize(Array(
    (&amp;quot;one&amp;quot;, &amp;quot;A&amp;quot;, 1), (&amp;quot;one&amp;quot;, &amp;quot;B&amp;quot;, 2), (&amp;quot;two&amp;quot;, &amp;quot;A&amp;quot;, 3), (&amp;quot;two&amp;quot;, &amp;quot;B&amp;quot;, 4)
)).toDF(&amp;quot;key1&amp;quot;, &amp;quot;key2&amp;quot;, &amp;quot;value&amp;quot;)
df.show()

+----+----+-----+
|key1|key2|value|
+----+----+-----+
| one|   A|    1|
| one|   B|    2|
| two|   A|    3|
| two|   B|    4|
+----+----+-----+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.GroupedData&#34;&gt;GroupedData.pivot&lt;/a&gt; allows making values from columns &lt;em&gt;key1&lt;/em&gt; or &lt;em&gt;key2&lt;/em&gt; new columns.
For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;df.groupBy(&amp;quot;key1&amp;quot;).pivot(&amp;quot;key2&amp;quot;).sum(&amp;quot;value&amp;quot;).show()

+----+-+-+
|key1|A|B|
+----+-+-+
| one|1|2|
| two|3|4|
+----+-+-+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Usually, data to be pivoted is not big and to avoid reshuffling, it makes sense to use &lt;code&gt;coalesce&lt;/code&gt; first.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Better to combine data within one data node before pivot
val groupedData = df.groupBy(&amp;quot;key1&amp;quot;).coalesce(1).cache()
groupedData.pivot(&amp;quot;key2&amp;quot;).sum(&amp;quot;value&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-standard-deviation-calculation:8ab4cd76c2e48d03608ec5aca1374ea6&#34;&gt;3. Standard deviation calculation&lt;/h3&gt;

&lt;p&gt;Spark is not yet mature in terms of statistics calculation. For example, it does not allow to &lt;a href=&#34;https://stackoverflow.com/questions/28158729/how-can-i-calculate-exact-median-with-apache-spark#answer-28160731&#34;&gt;calculate the median&lt;/a&gt; value of the column. One of the reasons is that linear algorithm could not be generalized to distributed RDD.&lt;/p&gt;

&lt;p&gt;Simple standard deviation was introduced only in spark 1.6.
A Potential problem with custom calculation could be with type overflow.
Example of usage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;df.agg(stddev(&amp;quot;value&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-simplified-outer-join:8ab4cd76c2e48d03608ec5aca1374ea6&#34;&gt;4. Simplified outer join&lt;/h3&gt;

&lt;p&gt;Join operation is essential for data manipulation and filtering in both RDBMS and distributed systems.
There are different types of joins, such as inner, left outer, right outer, semi, etc.
While inner join of data was relatively easy in earlier versions of spark, all of the other types required to specify join expression.
There are even more difficulties if join uses two or more columns.&lt;/p&gt;

&lt;p&gt;Join expressions are not that easy because they require additional DataFrame manipulations, such as column rename and further drop.
If the column has the same name in both data frames, it would not be dropped automatically and cause problems with future select.&lt;/p&gt;

&lt;p&gt;Suppose we have another DataFrame&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val df2 = sc.parallelize(Array(
    (&amp;quot;one&amp;quot;, &amp;quot;A&amp;quot;, 5), (&amp;quot;two&amp;quot;, &amp;quot;A&amp;quot;, 6)
)).toDF(&amp;quot;key1&amp;quot;, &amp;quot;key2&amp;quot;, &amp;quot;value2&amp;quot;)
df2.show()

+----+----+------+
|key1|key2|value2|
+----+----+------+
| one|   A|     5|
| two|   A|     6|
+----+----+------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Outer join prior to 1.6 could only be done using join expression:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val joined = df.join(df2, df(&amp;quot;key1&amp;quot;) === df2(&amp;quot;key1&amp;quot;) &amp;amp;&amp;amp; df(&amp;quot;key2&amp;quot;) === df2(&amp;quot;key2&amp;quot;), &amp;quot;left_outer&amp;quot;)
joined.show()

+----+----+-----+----+----+------+
|key1|key2|value|key1|key2|value2|
+----+----+-----+----+----+------+
| two|   A|    3| two|   A|     6|
| two|   B|    4|null|null|  null|
| one|   A|    1| one|   A|     5|
| one|   B|    2|null|null|  null|
+----+----+-----+----+----+------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Result data frame has duplicated column names, any operations with them would throw an error&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;joined.select(&amp;quot;key2&amp;quot;)

org.apache.spark.sql.AnalysisException: Reference &#39;key2&#39; is ambiguous, could be: key2#28, key2#34.;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To avoid duplication, one possible to rename columns before and drop them after the join.
The code in this case becomes messy and requires explanation.
Spark 1.6 simplifies &lt;a href=&#34;https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame&#34;&gt;join&lt;/a&gt; and allows to write&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;df.join(df2, Seq(&amp;quot;key1&amp;quot;, &amp;quot;key2&amp;quot;), &amp;quot;left_outer&amp;quot;).show()

+----+----+-----+------+
|key1|key2|value|value2|
+----+----+-----+------+
| two|   A|    3|     6|
| two|   B|    4|  null|
| one|   A|    1|     5|
| one|   B|    2|  null|
+----+----+-----+------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This syntax is much easier to read.&lt;/p&gt;

&lt;h3 id=&#34;5-quantilediscretizer-feature-transformer:8ab4cd76c2e48d03608ec5aca1374ea6&#34;&gt;5. QuantileDiscretizer feature transformer&lt;/h3&gt;

&lt;p&gt;Feature engineering is a big part of data mining.
Usually, data scientists try a lot of different approaches and at the end run some black box algorithm, such as random forest or xgboost.
The more features generated in the beginning, the better.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.QuantileDiscretizer&#34;&gt;QuantileDiscretizer&lt;/a&gt; is still experimental, but already available.
It allows splitting feature into buckets, based on value&amp;rsquo;s quantiles.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:8ab4cd76c2e48d03608ec5aca1374ea6&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Apache Spark is a dynamic project, every version brings a lot of new features.
Despite it offers excellent data manipulation tools, it is still quite weak in terms of data mining.
Spark niche is a Big Data, where familiar techniques might simply not work.&lt;/p&gt;

&lt;p&gt;It is worthwhile to follow Spark updates.
Based on several previous versions, every one of them brought significant functionality to the tool.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>