<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kirill Pavlov</title>
    <link>http://kirillpavlov.com/</link>
    <description>Recent content on Kirill Pavlov</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 10 Sep 2016 20:15:41 +0800</lastBuildDate>
    <atom:link href="http://kirillpavlov.com/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Lookup table maintenance in Hive</title>
      <link>http://kirillpavlov.com/blog/2016/09/10/lookup-table-maintenance-in-hive/</link>
      <pubDate>Sat, 10 Sep 2016 20:15:41 +0800</pubDate>
      
      <guid>http://kirillpavlov.com/blog/2016/09/10/lookup-table-maintenance-in-hive/</guid>
      <description>


&lt;figure &gt;
    
        &lt;img src=&#34;http://kirillpavlov.com/images/hive-lookup-tables-logo.png&#34; alt=&#34;data example&#34; /&gt;
    
    
&lt;/figure&gt;



&lt;p&gt;A &lt;em&gt;lookup table&lt;/em&gt; is a translation table, aimed to enrich and extend base data.
Such tables are very common, especially in data warehousing (schema normalisation) and business analytics area.
Usually, they are updated manually and developers are constantly looking for the ways to simplify maintenance.&lt;/p&gt;

&lt;p&gt;This article shows how to work with lookup tables in Hive using National Hockey League open data.&lt;/p&gt;

&lt;h3 id=&#34;data:4f2c28bcc32a95a1368ce77f1756dc27&#34;&gt;Data&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s consider the following example: base table consists of &lt;a href=&#34;http://sharks.nhl.com/club/page.htm?id=81102&#34;&gt;San Jose Sharks 2016-2017 NHL season schedule&lt;/a&gt;.
The table consists of records with game date, start time and description of a competitor.
Initial CSV file with schedule has the following format:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://kirillpavlov.com/images/san-jose-nhl-schedule-2016-2017-example.png&#34; alt=&#34;data example&#34; /&gt;
    
    
&lt;/figure&gt;



&lt;p&gt;Each team in NHL belongs to one of the four divisions, for example, San Jose is in Pacific.
Teams within the same division play more often than teams from different divisions.
Our goal is to analyse, how many games are played against each of four divisions teams.&lt;/p&gt;

&lt;p&gt;Base table with the schedule is located in a database already, but it does not have information about divisions.
If data is small, then one can use SQL expression to add necessary information: &amp;ldquo;if team == xxx then division = yyy&amp;rdquo;
However this is not the case. There are 30 teams in NHL and nobody would write such a big query.&lt;/p&gt;

&lt;p&gt;A way to go is to manually create a small table with team-to-division mapping and join it with original data.
The other question is in maintenance – even small data needs to be updated time to time.
One of the best tool to use for lookup data manipulation is Excel because it exists nearly everywhere and a lot of people know how to work with it.
Data from Excel is exported to CSV and uploaded to Hadoop.
This is a general idea, let&amp;rsquo;s talk about the details.&lt;/p&gt;

&lt;h3 id=&#34;schedule-data-in-hadoop:4f2c28bcc32a95a1368ce77f1756dc27&#34;&gt;Schedule data in Hadoop&lt;/h3&gt;

&lt;p&gt;The data is taken from official NHL site and uploaded to Hadoop as a CSV file.
To read it I use &lt;a href=&#34;http://github.com/databricks/spark-csv&#34;&gt;spark-csv&lt;/a&gt; library.
In order to extract dates from strings and extract actual competitor name, I use the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val schedule = sqlContext.read
  .format(&amp;quot;com.databricks.spark.csv&amp;quot;)
  .option(&amp;quot;header&amp;quot;, &amp;quot;true&amp;quot;)
  .option(&amp;quot;inferSchema&amp;quot;, &amp;quot;true&amp;quot;)
  .load(&amp;quot;lookup-example/san-jose-schedule-2016-2017.csv&amp;quot;)
  .select(
    to_date(
      unix_timestamp($&amp;quot;START_DATE&amp;quot;, &amp;quot;MM/dd/yyyy&amp;quot;).cast(&amp;quot;timestamp&amp;quot;)
    ) as &amp;quot;date&amp;quot;,
    when(
      locate(&amp;quot;San Jose&amp;quot;, $&amp;quot;SUBJECT&amp;quot;) === 1,
      regexp_extract($&amp;quot;SUBJECT&amp;quot;, &amp;quot;^San Jose at (.*)$&amp;quot;, 1)
    ).otherwise(
      regexp_extract($&amp;quot;SUBJECT&amp;quot;, &amp;quot;^(.*) at San Jose$&amp;quot;, 1)
    ) as &amp;quot;competitor&amp;quot;
  )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, schedule DataFrame has two columns: date and competitor.&lt;/p&gt;

&lt;h3 id=&#34;lookup-table:4f2c28bcc32a95a1368ce77f1756dc27&#34;&gt;Lookup table&lt;/h3&gt;

&lt;p&gt;To add division information to each team we would create a table &lt;em&gt;teams.xlsx&lt;/em&gt; in Excel (from &lt;a href=&#34;https://en.wikipedia.org/wiki/National_Hockey_League#List_of_teams&#34;&gt;Wikipedia&lt;/a&gt;):&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://kirillpavlov.com/images/lookup-example-nhl-teams.png&#34; alt=&#34;lookup table example&#34; /&gt;
    
    
&lt;/figure&gt;



&lt;p&gt;Then one need to upload exported &lt;em&gt;teams.csv&lt;/em&gt; to Hadoop as shown on the workflow below:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://kirillpavlov.com/images/lookup-table-hive.png&#34; alt=&#34;lookup table example&#34; /&gt;
    
    
&lt;/figure&gt;



&lt;p&gt;Next step is to add lookup data to Hive. It ensures that schema is persistent, so data update would not change it.
One may possible to read lookup table with spark-csv as we did with base table, but every single time it would require proper type cast if a schema is not inferred correctly.&lt;/p&gt;

&lt;p&gt;Since the data is already stored in Hadoop, there is no need to copy it to Hive. External table would work:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-hive&#34;&gt;CREATE EXTERNAL TABLE IF NOT EXISTS lookup_example_nhl_ext(
    team String,
    division String,
    conference String)
  COMMENT &#39;NHL teams&#39;
  ROW FORMAT DELIMITED
  FIELDS TERMINATED BY &#39;,&#39;
  LINES TERMINATED BY &#39;\n&#39;
  STORED AS TEXTFILE
  LOCATION &#39;hdfs:///user/&amp;lt;user&amp;gt;/lookup-example/nhl-lookup&#39;
  TBLPROPERTIES (&amp;quot;skip.header.line.count&amp;quot;=&amp;quot;1&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note, that&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;LOCATION field should have full file address in Hadoop,&lt;/li&gt;
&lt;li&gt;LOCATION address is a folder with one CSV file in it,&lt;/li&gt;
&lt;li&gt;Data header should be skipped in table properties (&amp;ldquo;skip.header.line.count&amp;rdquo;=&amp;ldquo;1&amp;rdquo;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This external table works fine in Hive&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-hive&#34;&gt;select * from lookup_example_nhl_ext limit 5;
OK
Boston Bruins Atlantic  Eastern Conference
Buffalo Sabres Atlantic Eastern Conference
Detroit Red Wings   Atlantic    Eastern Conference
Florida Panthers    Atlantic    Eastern Conference
Montreal Canadiens  Atlantic    Eastern Conference
Time taken: 0.1 seconds, Fetched: 5 row(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But it does not work with &lt;code&gt;HiveContext&lt;/code&gt; in spark. For some reason it keeps header row (Team,Division,Conference):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;sqlContext.table(&amp;quot;lookup_example_nhl_ext&amp;quot;).limit(2).show()

+-------------+--------+------------------+
|         team|division|        conference|
+-------------+--------+------------------+
|         Team|Division|        Conference|
|Boston Bruins|Atlantic|Eastern Conference|
+-------------+--------+------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One way to fix it is to create a Hive view, which filters the data.
Based on my observations, data warehouse techniques are penetrating into Hive (which is an unstructured data warehouse, by the way).
One of them is to maintain source tables as they are and read the data only from views. It aims to simplify data structure update.
Because of the same reason, I advocate Hive views, even if they implement simple logic, such as in our case:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-hive&#34;&gt;CREATE VIEW IF NOT EXISTS lookup_example_nhl
AS
SELECT *
FROM lookup_example_nhl_ext
WHERE team != &#39;Team&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now &lt;code&gt;HiveContext&lt;/code&gt; reads the data without header, so the issue is fixed.&lt;/p&gt;

&lt;h3 id=&#34;join-data-with-lookup-table:4f2c28bcc32a95a1368ce77f1756dc27&#34;&gt;Join data with lookup table&lt;/h3&gt;

&lt;p&gt;At this point of time, it should be easy to join the data and get the result.
However, schedule and lookup tables have different team names: base table has either city or competitor name.
To ensure that all of the code examples work here is a fix: add &lt;code&gt;short_name&lt;/code&gt; to the lookup table, which is either first, last or first two words from the team name.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val teams = sqlContext.table(&amp;quot;lookup_example_nhl&amp;quot;)
  .withColumn(&amp;quot;short_name&amp;quot;,
    when(
      locate(&amp;quot;New York&amp;quot;, $&amp;quot;team&amp;quot;) === 1,
      regexp_extract($&amp;quot;team&amp;quot;, &amp;quot;\\w+$&amp;quot;, 0)
    ).when(
      (locate(&amp;quot;Devils&amp;quot;, $&amp;quot;team&amp;quot;) &amp;gt; 0) ||
      (locate(&amp;quot;Kings&amp;quot;, $&amp;quot;team&amp;quot;) &amp;gt; 0) ||
      (locate(&amp;quot;Sharks&amp;quot;, $&amp;quot;team&amp;quot;) &amp;gt; 0) ||
      (locate(&amp;quot;Blues&amp;quot;, $&amp;quot;team&amp;quot;) &amp;gt; 0),
      regexp_extract($&amp;quot;team&amp;quot;, &amp;quot;^(.*) \\w+&amp;quot;, 1)
    ).otherwise(regexp_extract($&amp;quot;team&amp;quot;, &amp;quot;^\\w+&amp;quot;, 0))
  )

teams.show()

+--------------------+------------+------------------+------------+
|                team|    division|        conference|  short_name|
+--------------------+------------+------------------+------------+
|       Boston Bruins|    Atlantic|Eastern Conference|      Boston|
|      Buffalo Sabres|    Atlantic|Eastern Conference|     Buffalo|
|   Detroit Red Wings|    Atlantic|Eastern Conference|     Detroit|
|    Florida Panthers|    Atlantic|Eastern Conference|     Florida|
|  Montreal Canadiens|    Atlantic|Eastern Conference|    Montreal|
|     Ottawa Senators|    Atlantic|Eastern Conference|      Ottawa|
| Tampa Bay Lightning|    Atlantic|Eastern Conference|       Tampa|
| Toronto Maple Leafs|    Atlantic|Eastern Conference|     Toronto|
| Carolina Hurricanes|Metropolitan|Eastern Conference|    Carolina|
|Columbus Blue Jac...|Metropolitan|Eastern Conference|    Columbus|
+--------------------+------------+------------------+------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now &lt;code&gt;short_name&lt;/code&gt; is almost the same as &lt;code&gt;competitor&lt;/code&gt;.
One need to do a &amp;ldquo;fuzzy&amp;rdquo; join.
Check out &lt;a href=&#34;http://kirillpavlov.com/blog/2016/04/23/beyond-traditional-join-with-apache-spark/#join-expression-slowly-changing-dimensions-and-non-equi-join:58684124e2b27686159fe7ccddda41aa&#34;&gt;join expressions&lt;/a&gt; for more information.
We would do join based on &lt;a href=&#34;https://en.wikipedia.org/wiki/Levenshtein_distance&#34;&gt;Levenshtein distance&lt;/a&gt;, which is 0 for exactly the same strings and small for nearly the same.
There is no golden rule for such join, but initial distance should be big enough to find fuzzy lookup match and small enough to not cartesian-join both tables.
Result table could have several lookup teams for one original. To filter wrong matches, we would use window function, which keeps only match with lower Levenshtein distance.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.Window

val competitorWindow = Window
  .partitionBy(&amp;quot;date&amp;quot;, &amp;quot;competitor&amp;quot;)
  .orderBy(levenshtein($&amp;quot;competitor&amp;quot;, $&amp;quot;short_name&amp;quot;))

val scheduleRich = schedule
  .join(
    teams, levenshtein($&amp;quot;competitor&amp;quot;, $&amp;quot;short_name&amp;quot;) &amp;lt; 5, &amp;quot;left_outer&amp;quot;
  )
  .withColumn(&amp;quot;_rank&amp;quot;, row_number().over(competitorWindow))
  .filter($&amp;quot;_rank&amp;quot; === 1)
  .drop(&amp;quot;_rank&amp;quot;)

scheduleRich.drop(&amp;quot;short_name&amp;quot;).show(2)

+----------+----------+---------------+--------+------------------+
|      date|competitor|           team|division|        conference|
+----------+----------+---------------+--------+------------------+
|2016-10-05|   Anaheim|  Anaheim Ducks| Pacific|Western Conference|
|2016-10-09|   Anaheim|  Anaheim Ducks| Pacific|Western Conference|
+----------+----------+---------------+--------+------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, schedule data is enriched and one may possible to compute the result:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;scheduleRich.groupBy(&amp;quot;division&amp;quot;).count().orderBy($&amp;quot;count&amp;quot;.desc).show()

+------------+-----+
|    division|count|
+------------+-----+
|     Pacific|   36|
|     Central|   20|
|    Atlantic|   16|
|Metropolitan|   16|
+------------+-----+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As expected, San Jose plays more games against other teams from Pacific division, but there are also slightly more games against Central division teams.
Did you know it before?&lt;/p&gt;

&lt;h3 id=&#34;conclusion:4f2c28bcc32a95a1368ce77f1756dc27&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;This tutorial has shown, how to deal with lookup tables with Hadoop and Hive, including header row fix.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Beyond traditional join with Apache Spark</title>
      <link>http://kirillpavlov.com/blog/2016/04/23/beyond-traditional-join-with-apache-spark/</link>
      <pubDate>Sat, 23 Apr 2016 09:40:05 +0800</pubDate>
      
      <guid>http://kirillpavlov.com/blog/2016/04/23/beyond-traditional-join-with-apache-spark/</guid>
      <description>

&lt;p&gt;An &lt;a href=&#34;https://en.wikipedia.org/wiki/Join_(SQL)&#34;&gt;SQL join&lt;/a&gt; clause combines records from two or more tables.
This operation is very common in data processing and understanding of what happens under the hood is important.
There are several common join types: &lt;code&gt;INNER&lt;/code&gt;, &lt;code&gt;LEFT OUTER&lt;/code&gt;, &lt;code&gt;RIGHT OUTER&lt;/code&gt;, &lt;code&gt;FULL OUTER&lt;/code&gt; and &lt;code&gt;CROSS&lt;/code&gt; or &lt;code&gt;CARTESIAN&lt;/code&gt;.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://kirillpavlov.com/images/join-types.png&#34; alt=&#34;join types&#34; /&gt;
    
    
&lt;/figure&gt;



&lt;p&gt;Join which uses the same table is a self-join.
If an operation uses equality operator, it is &lt;a href=&#34;https://en.wikipedia.org/wiki/Join_(SQL)#Equi-join&#34;&gt;equi-join&lt;/a&gt;, otherwise, it is non-equi-join.&lt;/p&gt;

&lt;p&gt;This article covers different join types in Apache Spark as well as examples of &lt;a href=&#34;https://en.wikipedia.org/wiki/Slowly_changing_dimension&#34;&gt;slowly changed dimensions (SCD)&lt;/a&gt; and joins on non-unique columns.&lt;/p&gt;

&lt;h3 id=&#34;sample-data:58684124e2b27686159fe7ccddda41aa&#34;&gt;Sample data&lt;/h3&gt;

&lt;p&gt;All subsequent explanations on join types in this article make use of the following two tables, taken from Wikipedia article.
The rows in these tables serve to illustrate the effect of different types of joins and join-predicates.&lt;/p&gt;

&lt;p&gt;Employees table has a nullable column. To express it in terms of statically typed Scala, one needs to use &lt;a href=&#34;http://www.scala-lang.org/api/current/#scala.Option&#34;&gt;Option&lt;/a&gt; type.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val employees = sc.parallelize(Array[(String, Option[Int])](
  (&amp;quot;Rafferty&amp;quot;, Some(31)), (&amp;quot;Jones&amp;quot;, Some(33)), (&amp;quot;Heisenberg&amp;quot;, Some(33)), (&amp;quot;Robinson&amp;quot;, Some(34)), (&amp;quot;Smith&amp;quot;, Some(34)), (&amp;quot;Williams&amp;quot;, null)
)).toDF(&amp;quot;LastName&amp;quot;, &amp;quot;DepartmentID&amp;quot;)

employees.show()

+----------+------------+
|  LastName|DepartmentID|
+----------+------------+
|  Rafferty|          31|
|     Jones|          33|
|Heisenberg|          33|
|  Robinson|          34|
|     Smith|          34|
|  Williams|        null|
+----------+------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Department table does not have nullable columns, type specification could be omitted.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val departments = sc.parallelize(Array(
  (31, &amp;quot;Sales&amp;quot;), (33, &amp;quot;Engineering&amp;quot;), (34, &amp;quot;Clerical&amp;quot;),
  (35, &amp;quot;Marketing&amp;quot;)
)).toDF(&amp;quot;DepartmentID&amp;quot;, &amp;quot;DepartmentName&amp;quot;)

departments.show()

+------------+--------------+
|DepartmentID|DepartmentName|
+------------+--------------+
|          31|         Sales|
|          33|   Engineering|
|          34|      Clerical|
|          35|     Marketing|
+------------+--------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;inner-join:58684124e2b27686159fe7ccddda41aa&#34;&gt;Inner join&lt;/h3&gt;

&lt;p&gt;Following SQL code&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-SQL&#34;&gt;SELECT *
FROM employee 
INNER JOIN department
ON employee.DepartmentID = department.DepartmentID;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;could be written in Spark as&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;employees
  .join(departments, &amp;quot;DepartmentID&amp;quot;)
  .show()

+------------+----------+--------------+
|DepartmentID|  LastName|DepartmentName|
+------------+----------+--------------+
|          31|  Rafferty|         Sales|
|          33|     Jones|   Engineering|
|          33|Heisenberg|   Engineering|
|          34|  Robinson|      Clerical|
|          34|     Smith|      Clerical|
+------------+----------+--------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Beautiful, is not it? Spark automatically removes duplicated &amp;ldquo;DepartmentID&amp;rdquo; column, so column names are unique and one does not need to use table prefix to address them.&lt;/p&gt;

&lt;h3 id=&#34;left-outer-join:58684124e2b27686159fe7ccddda41aa&#34;&gt;Left outer join&lt;/h3&gt;

&lt;p&gt;Left outer join is a very common operation, especially if there are nulls or gaps in a data.
Note, that column name should be wrapped into scala &lt;code&gt;Seq&lt;/code&gt; if join type is specified.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;employees
  .join(departments, Seq(&amp;quot;DepartmentID&amp;quot;), &amp;quot;left_outer&amp;quot;)
  .show()

+------------+----------+--------------+
|DepartmentID|  LastName|DepartmentName|
+------------+----------+--------------+
|          31|  Rafferty|         Sales|
|          33|     Jones|   Engineering|
|          33|Heisenberg|   Engineering|
|          34|  Robinson|      Clerical|
|          34|     Smith|      Clerical|
|        null|  Williams|          null|
+------------+----------+--------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;other-join-types:58684124e2b27686159fe7ccddda41aa&#34;&gt;Other join types&lt;/h3&gt;

&lt;p&gt;Spark allows using following join types: &lt;code&gt;inner&lt;/code&gt;, &lt;code&gt;outer&lt;/code&gt;, &lt;code&gt;left_outer&lt;/code&gt;, &lt;code&gt;right_outer&lt;/code&gt;, &lt;code&gt;leftsemi&lt;/code&gt;.
The interface is the same as for left outer join in the example above.&lt;/p&gt;

&lt;p&gt;For cartesian join column specification should be omitted:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;employees
  .join(departments)
  .show(10)

+----------+------------+------------+--------------+
|  LastName|DepartmentID|DepartmentID|DepartmentName|
+----------+------------+------------+--------------+
|  Rafferty|          31|          31|         Sales|
|  Rafferty|          31|          33|   Engineering|
|  Rafferty|          31|          34|      Clerical|
|  Rafferty|          31|          35|     Marketing|
|     Jones|          33|          31|         Sales|
|     Jones|          33|          33|   Engineering|
|     Jones|          33|          34|      Clerical|
|     Jones|          33|          35|     Marketing|
|Heisenberg|          33|          31|         Sales|
|Heisenberg|          33|          33|   Engineering|
+----------+------------+------------+--------------+
only showing top 10 rows
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Warning: do not use cartesian join with big tables in production.&lt;/p&gt;

&lt;h3 id=&#34;join-expression-slowly-changing-dimensions-and-non-equi-join:58684124e2b27686159fe7ccddda41aa&#34;&gt;Join expression, slowly changing dimensions and non-equi join&lt;/h3&gt;

&lt;p&gt;Spark allows us to specify join expression instead of a sequence of columns.
In general, expression specification is less readable, so why do we need such flexibility?
The reason is non-equi join.&lt;/p&gt;

&lt;p&gt;One application of it is slowly changing dimensions.
Assume there is a table with product prices over time:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val products = sc.parallelize(Array(
  (&amp;quot;steak&amp;quot;, &amp;quot;1990-01-01&amp;quot;, &amp;quot;2000-01-01&amp;quot;, 150),
  (&amp;quot;steak&amp;quot;, &amp;quot;2000-01-02&amp;quot;, &amp;quot;2020-01-01&amp;quot;, 180),
  (&amp;quot;fish&amp;quot;, &amp;quot;1990-01-01&amp;quot;, &amp;quot;2020-01-01&amp;quot;, 100)
)).toDF(&amp;quot;name&amp;quot;, &amp;quot;startDate&amp;quot;, &amp;quot;endDate&amp;quot;, &amp;quot;price&amp;quot;)

products.show()

+-----+----------+----------+-----+
| name| startDate|   endDate|price|
+-----+----------+----------+-----+
|steak|1990-01-01|2000-01-01|  150|
|steak|2000-01-02|2020-01-01|  180|
| fish|1990-01-01|2020-01-01|  100|
+-----+----------+----------+-----+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are two products only: steak and fish, price of steak has been changed once.
Another table consists of product orders by day:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val orders = sc.parallelize(Array(
  (&amp;quot;1995-01-01&amp;quot;, &amp;quot;steak&amp;quot;),
  (&amp;quot;2000-01-01&amp;quot;, &amp;quot;fish&amp;quot;),
  (&amp;quot;2005-01-01&amp;quot;, &amp;quot;steak&amp;quot;)
)).toDF(&amp;quot;date&amp;quot;, &amp;quot;product&amp;quot;)

orders.show()

+----------+-------+
|      date|product|
+----------+-------+
|1995-01-01|  steak|
|2000-01-01|   fish|
|2005-01-01|  steak|
+----------+-------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our goal is to assign an actual price for every record in the orders table.
It is not obvious to do using only equality operators, however, spark join expression allows us to achieve the result in an elegant way:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;orders
  .join(products, $&amp;quot;product&amp;quot; === $&amp;quot;name&amp;quot; &amp;amp;&amp;amp; $&amp;quot;date&amp;quot; &amp;gt;= $&amp;quot;startDate&amp;quot; &amp;amp;&amp;amp; $&amp;quot;date&amp;quot; &amp;lt;= $&amp;quot;endDate&amp;quot;)
  .show()

+----------+-------+-----+----------+----------+-----+
|      date|product| name| startDate|   endDate|price|
+----------+-------+-----+----------+----------+-----+
|2000-01-01|   fish| fish|1990-01-01|2020-01-01|  100|
|1995-01-01|  steak|steak|1990-01-01|2000-01-01|  150|
|2005-01-01|  steak|steak|2000-01-02|2020-01-01|  180|
+----------+-------+-----+----------+----------+-----+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This technique is very useful, yet not that common.
It could save a lot of time for those who write as well as for those who read the code.&lt;/p&gt;

&lt;h3 id=&#34;inner-join-using-non-primary-keys:58684124e2b27686159fe7ccddda41aa&#34;&gt;Inner join using non primary keys&lt;/h3&gt;

&lt;p&gt;Last part of this article is about joins on non unique columns and common mistakes related to it.
Join (intersection) diagrams in the beginning of this article stuck in our heads.
Because of visual comparison of sets intersection we assume, that result table after inner join should be smaller, than any of the source tables.
This is correct only for joins on unique columns and wrong if columns in both tables are not unique.
Consider following DataFrame with duplicated records and its self-join:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val df = sc.parallelize(Array(
  (0), (1), (1)
)).toDF(&amp;quot;c1&amp;quot;)

df.show()
df.join(df, &amp;quot;c1&amp;quot;).show()

// Original DataFrame
+---+
| c1|
+---+
|  0|
|  1|
|  1|
+---+

// Self-joined DataFrame
+---+
| c1|
+---+
|  0|
|  1|
|  1|
|  1|
|  1|
+---+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note, that size of the result DataFrame is bigger than the source size. It could be as big as n&lt;sup&gt;2&lt;/sup&gt;, where n is a size of source.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:58684124e2b27686159fe7ccddda41aa&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;The article covered different join types implementations with Apache Spark, including join expressions and join on non-unique keys.&lt;/p&gt;

&lt;p&gt;Apache Spark allows developers to write the code in the way, which is easier to understand. It improves code quality and maintainability.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Immutable heap implementation in Scala</title>
      <link>http://kirillpavlov.com/blog/2016/02/28/immutable-heap-implementation-in-scala/</link>
      <pubDate>Sun, 28 Feb 2016 21:32:38 +0800</pubDate>
      
      <guid>http://kirillpavlov.com/blog/2016/02/28/immutable-heap-implementation-in-scala/</guid>
      <description>&lt;p&gt;Current &lt;a href=&#34;https://en.wikipedia.org/wiki/Heap_(data_structure)&#34;&gt;Heap&lt;/a&gt; implementation in Scala (&lt;a href=&#34;https://github.com/scala/scala/blob/2.12.x/src/library/scala/collection/mutable/PriorityQueue.scala&#34;&gt;PriorityQueue&lt;/a&gt;) is mutable. It means that after heap manipulation, the previous state is no longer accessible. This article describes immutable heap construction based on Scala Vector.&lt;/p&gt;

&lt;p&gt;First of all, we need to define an interface to the Heap. It should have &lt;code&gt;insert&lt;/code&gt; and &lt;code&gt;extract&lt;/code&gt; methods.
As far as designed data structure should be immutable, both methods should return the whole heap in addition to expected result.&lt;/p&gt;

&lt;p&gt;There are two helper methods: &lt;code&gt;siftUp&lt;/code&gt; and &lt;code&gt;siftDown&lt;/code&gt;, which help to fix heap property.
Suppose we store keys in &lt;code&gt;Vector&lt;/code&gt; of type &lt;code&gt;T&lt;/code&gt; with implicit ordering:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;class Heap[T: Ordering](val keys: Vector[T]) {
  val keyOrdering = implicitly[Ordering[T]]
  ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This definition allows us to use arbitrary type for stored objects, including (key, value) pair and use the same code for min and max heap.&lt;/p&gt;

&lt;p&gt;Method &lt;code&gt;siftDown&lt;/code&gt; moves an element with greater value than it&amp;rsquo;s children down.
Every time it selects child with minimal value and swaps current element with it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;private def siftDownKeys(keys: Vector[T], i: Int): Vector[T] = {
  while (2 * i + 1 &amp;lt; size) {
    val left = 2 * i + 1  // left child
    val right = left + 1  // right child
    var j = left
    if (right &amp;lt; size &amp;amp;&amp;amp; keyOrdering.compare(keys(right), keys(left)) &amp;lt; 0) {j = right}
    if (keyOrdering.compare(keys(i), keys(j)) &amp;lt;= 0) return keys
    return siftDownKeys(swap(keys, i, j), j)
  }
  keys
}

private def siftDown(i: Int): Heap[T] = new Heap(siftDownKeys(keys, i))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Method &lt;code&gt;siftUp&lt;/code&gt; moves an element with smaller value than it&amp;rsquo;s parent up.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;private def siftUpKeys(keys: Vector[T], i: Int): Vector[T] = {
  val j = (i - 1) / 2
  while (keyOrdering.compare(keys(i), keys(j)) &amp;lt; 0)
    return siftUpKeys(swap(keys, i, j), j)
  keys
}

private def siftUp(i: Int): Heap[T] = new Heap(siftUpKeys(keys, i))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using these helper methods, it is easy to implement defined interface methods:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def insert(key: T): Heap[T] = new Heap(keys :+ key) siftUp size
def extract(): (T, Heap[T]) = (
  keys(0),
  new Heap(keys.last +: keys.tail.dropRight(1)) siftDown 0
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Final &lt;a href=&#34;https://github.com/pavlov99/scalastructures/blob/3a938f9402ed0609c93bbfbb59e3fc83798969fc/src/main/scala/com/github/pavlov99/heap/Heap.scala&#34;&gt;implementation&lt;/a&gt; takes 74 lines, which is less than default one.
Performance is worst compared to mutable version because of the data manipulation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to update your maven</title>
      <link>http://kirillpavlov.com/blog/2016/02/22/how-to-update-your-maven/</link>
      <pubDate>Mon, 22 Feb 2016 20:09:47 +0800</pubDate>
      
      <guid>http://kirillpavlov.com/blog/2016/02/22/how-to-update-your-maven/</guid>
      <description>&lt;p&gt;Maven is a popular build manager in java world. It is used to build Apache Spark, for example.
Here is correct deb repository with latest version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get purge maven maven2 maven3
sudo apt-add-repository ppa:andrei-pozolotin/maven3
sudo apt-get update
sudo apt-get install maven3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Version available at the time of writing is &lt;code&gt;3.3.9&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Top 5 features released in spark 1.6</title>
      <link>http://kirillpavlov.com/blog/2016/02/21/top-5-features-released-in-spark-1.6/</link>
      <pubDate>Sun, 21 Feb 2016 00:20:45 +0800</pubDate>
      
      <guid>http://kirillpavlov.com/blog/2016/02/21/top-5-features-released-in-spark-1.6/</guid>
      <description>

&lt;p&gt;Spark version 1.6 &lt;a href=&#34;https://spark.apache.org/releases/spark-release-1-6-0.html&#34;&gt;has been released&lt;/a&gt; on January 4th, 2016.
Compared to the previous version, it has significant improvements. This article covers top 5 of them.&lt;/p&gt;

&lt;h3 id=&#34;1-partition-by-column:8ab4cd76c2e48d03608ec5aca1374ea6&#34;&gt;1. Partition by column&lt;/h3&gt;

&lt;p&gt;The idea is to have more control on RDD&amp;rsquo;s partitioning.
Sometimes data needs to be joined and grouped by certain key, such as user_id.
To minify data reshuffling, one may possible to store chunks of objects with the same key within the same data node.&lt;/p&gt;

&lt;p&gt;This feature &lt;a href=&#34;https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SortBy#LanguageManualSortBy-SyntaxofClusterByandDistributeBy&#34;&gt;exists in Hive&lt;/a&gt; and has been &lt;a href=&#34;https://issues.apache.org/jira/browse/SPARK-11410&#34;&gt;ported to spark&lt;/a&gt;.
Example of usage:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val df = sc.parallelize(Array(
    (&amp;quot;A&amp;quot;, 1), (&amp;quot;B&amp;quot;, 2), (&amp;quot;C&amp;quot;, 3), (&amp;quot;A&amp;quot;, 4)
)).toDF(&amp;quot;key&amp;quot;, &amp;quot;value&amp;quot;)
val partitioned = df.repartition($&amp;quot;key&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-groupeddata-pivot:8ab4cd76c2e48d03608ec5aca1374ea6&#34;&gt;2. GroupedData Pivot&lt;/h3&gt;

&lt;p&gt;This feature is about data presentation: if we need to transform adjacency list to adjacency matrix or convert long narrow RDD to the matrix - pivot is our friend.
Python has pivot functionality in Pandas DataFrames: &lt;a href=&#34;http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.unstack.html#pandas.DataFrame.unstack&#34;&gt;unstack&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val df = sc.parallelize(Array(
    (&amp;quot;one&amp;quot;, &amp;quot;A&amp;quot;, 1), (&amp;quot;one&amp;quot;, &amp;quot;B&amp;quot;, 2), (&amp;quot;two&amp;quot;, &amp;quot;A&amp;quot;, 3), (&amp;quot;two&amp;quot;, &amp;quot;B&amp;quot;, 4)
)).toDF(&amp;quot;key1&amp;quot;, &amp;quot;key2&amp;quot;, &amp;quot;value&amp;quot;)
df.show()

+----+----+-----+
|key1|key2|value|
+----+----+-----+
| one|   A|    1|
| one|   B|    2|
| two|   A|    3|
| two|   B|    4|
+----+----+-----+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.GroupedData&#34;&gt;GroupedData.pivot&lt;/a&gt; allows making values from columns &lt;em&gt;key1&lt;/em&gt; or &lt;em&gt;key2&lt;/em&gt; new columns.
For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;df.groupBy(&amp;quot;key1&amp;quot;).pivot(&amp;quot;key2&amp;quot;).sum(&amp;quot;value&amp;quot;).show()

+----+-+-+
|key1|A|B|
+----+-+-+
| one|1|2|
| two|3|4|
+----+-+-+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Usually, data to be pivoted is not big and to avoid reshuffling, it makes sense to use &lt;code&gt;coalesce&lt;/code&gt; first.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;// Better to combine data within one data node before pivot
val groupedData = df.groupBy(&amp;quot;key1&amp;quot;).coalesce(1).cache()
groupedData.pivot(&amp;quot;key2&amp;quot;).sum(&amp;quot;value&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-standard-deviation-calculation:8ab4cd76c2e48d03608ec5aca1374ea6&#34;&gt;3. Standard deviation calculation&lt;/h3&gt;

&lt;p&gt;Spark is not yet mature in terms of statistics calculation. For example, it does not allow to &lt;a href=&#34;https://stackoverflow.com/questions/28158729/how-can-i-calculate-exact-median-with-apache-spark#answer-28160731&#34;&gt;calculate the median&lt;/a&gt; value of the column. One of the reasons is that linear algorithm could not be generalized to distributed RDD.&lt;/p&gt;

&lt;p&gt;Simple standard deviation was introduced only in spark 1.6.
A Potential problem with custom calculation could be with type overflow.
Example of usage:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;df.agg(stddev(&amp;quot;value&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-simplified-outer-join:8ab4cd76c2e48d03608ec5aca1374ea6&#34;&gt;4. Simplified outer join&lt;/h3&gt;

&lt;p&gt;Join operation is essential for data manipulation and filtering in both RDBMS and distributed systems.
There are different types of joins, such as inner, left outer, right outer, semi, etc.
While inner join of data was relatively easy in earlier versions of spark, all of the other types required to specify join expression.
There are even more difficulties if join uses two or more columns.&lt;/p&gt;

&lt;p&gt;Join expressions are not that easy because they require additional DataFrame manipulations, such as column rename and further drop.
If the column has the same name in both data frames, it would not be dropped automatically and cause problems with future select.&lt;/p&gt;

&lt;p&gt;Suppose we have another DataFrame&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val df2 = sc.parallelize(Array(
    (&amp;quot;one&amp;quot;, &amp;quot;A&amp;quot;, 5), (&amp;quot;two&amp;quot;, &amp;quot;A&amp;quot;, 6)
)).toDF(&amp;quot;key1&amp;quot;, &amp;quot;key2&amp;quot;, &amp;quot;value2&amp;quot;)
df2.show()

+----+----+------+
|key1|key2|value2|
+----+----+------+
| one|   A|     5|
| two|   A|     6|
+----+----+------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Outer join prior to 1.6 could only be done using join expression:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val joined = df.join(df2, df(&amp;quot;key1&amp;quot;) === df2(&amp;quot;key1&amp;quot;) &amp;amp;&amp;amp; df(&amp;quot;key2&amp;quot;) === df2(&amp;quot;key2&amp;quot;), &amp;quot;left_outer&amp;quot;)
joined.show()

+----+----+-----+----+----+------+
|key1|key2|value|key1|key2|value2|
+----+----+-----+----+----+------+
| two|   A|    3| two|   A|     6|
| two|   B|    4|null|null|  null|
| one|   A|    1| one|   A|     5|
| one|   B|    2|null|null|  null|
+----+----+-----+----+----+------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Result data frame has duplicated column names, any operations with them would throw an error&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;joined.select(&amp;quot;key2&amp;quot;)

org.apache.spark.sql.AnalysisException: Reference &#39;key2&#39; is ambiguous, could be: key2#28, key2#34.;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To avoid duplication, one possible to rename columns before and drop them after the join.
The code in this case becomes messy and requires explanation.
Spark 1.6 simplifies &lt;a href=&#34;https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame&#34;&gt;join&lt;/a&gt; and allows to write&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;df.join(df2, Seq(&amp;quot;key1&amp;quot;, &amp;quot;key2&amp;quot;), &amp;quot;left_outer&amp;quot;).show()

+----+----+-----+------+
|key1|key2|value|value2|
+----+----+-----+------+
| two|   A|    3|     6|
| two|   B|    4|  null|
| one|   A|    1|     5|
| one|   B|    2|  null|
+----+----+-----+------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This syntax is much easier to read.&lt;/p&gt;

&lt;h3 id=&#34;5-quantilediscretizer-feature-transformer:8ab4cd76c2e48d03608ec5aca1374ea6&#34;&gt;5. QuantileDiscretizer feature transformer&lt;/h3&gt;

&lt;p&gt;Feature engineering is a big part of data mining.
Usually, data scientists try a lot of different approaches and at the end run some black box algorithm, such as random forest or xgboost.
The more features generated in the beginning, the better.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.QuantileDiscretizer&#34;&gt;QuantileDiscretizer&lt;/a&gt; is still experimental, but already available.
It allows splitting feature into buckets, based on value&amp;rsquo;s quantiles.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:8ab4cd76c2e48d03608ec5aca1374ea6&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Apache Spark is a dynamic project, every version brings a lot of new features.
Despite it offers excellent data manipulation tools, it is still quite weak in terms of data mining.
Spark niche is a Big Data, where familiar techniques might simply not work.&lt;/p&gt;

&lt;p&gt;It is worthwhile to follow Spark updates.
Based on several previous versions, every one of them brought significant functionality to the tool.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Go versions, how to make updates easier</title>
      <link>http://kirillpavlov.com/blog/2016/02/20/go-versions-how-to-make-updates-easier/</link>
      <pubDate>Sat, 20 Feb 2016 23:06:18 +0800</pubDate>
      
      <guid>http://kirillpavlov.com/blog/2016/02/20/go-versions-how-to-make-updates-easier/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://golang.org/&#34;&gt;Go&lt;/a&gt; is an open source programming language that makes it easy to build simple, reliable, and efficient software.&lt;/p&gt;

&lt;p&gt;Because of its rapid development, there is an issue with version updates.
It requires not only download and compile new version, but also update &lt;code&gt;$GOROOT&lt;/code&gt; and &lt;code&gt;$GOPATH&lt;/code&gt; environment variables.&lt;/p&gt;

&lt;p&gt;One way to simplify this process is to use version manager, such as &lt;a href=&#34;https://github.com/moovweb/gvm&#34;&gt;gvm&lt;/a&gt;.
Installations process is super easy:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash &amp;lt; &amp;lt;(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To install proper version of Go use:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gvm install go1.6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As simple as that.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Get random lines from file with bash</title>
      <link>http://kirillpavlov.com/blog/2015/08/27/get-random-lines-from-file-with-bash/</link>
      <pubDate>Thu, 27 Aug 2015 06:18:14 +0000</pubDate>
      
      <guid>http://kirillpavlov.com/blog/2015/08/27/get-random-lines-from-file-with-bash/</guid>
      <description>

&lt;p&gt;Data sampling is one of the duties of data scientists and data engineers.
One may require to split original data into train and test subsets.
How could we do it fast with less amount of code?
This article shows usage of different command line tools for such task.&lt;/p&gt;

&lt;p&gt;There are several ways to get random lines from a file:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sort lines with random key&lt;/li&gt;
&lt;li&gt;&lt;code&gt;shuf&lt;/code&gt; from GNU core utils&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rl&lt;/code&gt; randomize-lines package&lt;/li&gt;
&lt;li&gt;perl one-liner&lt;/li&gt;
&lt;li&gt;python one-liner&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of the approaches would be compared in terms of execution time, tools availability and code complexity. File to be sorted consists of 10M lines:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;FILENAME=&amp;quot;/tmp/random-lines.$$.tmp&amp;quot;
NUMLINES=10000000
seq -f &#39;line %.0f&#39; $NUMLINES &amp;gt; $FILENAME;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;sort:7f16ea4a76eb101e46f16364e2326e6b&#34;&gt;sort&lt;/h2&gt;

&lt;p&gt;Default &lt;code&gt;sort&lt;/code&gt; has option &lt;code&gt;-R&lt;/code&gt;, &lt;code&gt;--random-sort&lt;/code&gt; which sorts lines by random hash. However, if there are two lines with the same content, their hashes would be the same and they would be sorted one after another. To prevent such case, one may possible to make all of the lines unique via adding line number to all of them.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nl -ba $FILENAME | sort -R | sed &#39;s/.*[0-9]\t//&#39; | head
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Time: 3 min 09.77 sec&lt;/p&gt;

&lt;p&gt;Complexity: medium, need to keep in mind making lines unique&lt;/p&gt;

&lt;p&gt;Availability: good&lt;/p&gt;

&lt;h2 id=&#34;shuf:7f16ea4a76eb101e46f16364e2326e6b&#34;&gt;shuf&lt;/h2&gt;

&lt;p&gt;Another bash tool &lt;code&gt;shuf&lt;/code&gt; on the other hand will sufficiently randomize a list, including not putting duplicate lines next to each other. Another advantage of this tool is it&amp;rsquo;s availability. Being part of GNU core utils it is available on nearly every machine.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;shuf $FILENAME | head
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It has parameter &lt;code&gt;-n&lt;/code&gt; to specify number of lines to output, however based on my tests, it does not speed up the process. Combination with &lt;code&gt;head&lt;/code&gt; works better.&lt;/p&gt;

&lt;p&gt;Time: 0.14 sec&lt;/p&gt;

&lt;p&gt;Complexity: easy&lt;/p&gt;

&lt;p&gt;Availability: good&lt;/p&gt;

&lt;h2 id=&#34;randomized-lines:7f16ea4a76eb101e46f16364e2326e6b&#34;&gt;randomized-lines&lt;/h2&gt;

&lt;p&gt;Tool &lt;code&gt;rl&lt;/code&gt; from &lt;a href=&#34;http://manpages.ubuntu.com/manpages/wily/en/man1/rl.1.html&#34;&gt;randomize-lines&lt;/a&gt; package makes random sampling easy, however, not every machine has it. As mentioned in it&amp;rsquo;s description: &amp;ldquo;It does this with only a single pass over the input while trying to use as little memory as possible&amp;rdquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rl $FILENAME | head
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Time: 0.68 sec&lt;/p&gt;

&lt;p&gt;Complexity: easy&lt;/p&gt;

&lt;p&gt;Availability: bad, need to install from external repository&lt;/p&gt;

&lt;h2 id=&#34;perl:7f16ea4a76eb101e46f16364e2326e6b&#34;&gt;perl&lt;/h2&gt;

&lt;p&gt;Perl is a good language for text processing. For those developers, who are less familiar with bash, it might be native to try it first.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat $FILENAME | perl -MList::Util=shuffle -e &#39;print shuffle(&amp;lt;STDIN&amp;gt;);&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Time: 2.11 sec&lt;/p&gt;

&lt;p&gt;Availability: medium, some of the machines might not have it&lt;/p&gt;

&lt;p&gt;Complexity: medium, need to remember how to call perl from bash and include libraries&lt;/p&gt;

&lt;h2 id=&#34;python:7f16ea4a76eb101e46f16364e2326e6b&#34;&gt;python&lt;/h2&gt;

&lt;p&gt;Python is among most popular programming languages. Nowadays it exists on nearly every machine and a lot of developers worked with it at least once. It has library to work with random numbers and shuffles. As well as perl, it could be invoked from bash.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -c &amp;quot;import random, sys; lines = open(sys.argv[1]).readlines(); random.shuffle(lines); print &#39;&#39;.join(lines),&amp;quot; $FILENAME
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Script execution is inefficient, it requires to store data in memory.&lt;/p&gt;

&lt;p&gt;Time: 6.92 sec&lt;/p&gt;

&lt;p&gt;Availability: medium, some of the machines might not have it&lt;/p&gt;

&lt;p&gt;Complexity: medium, need to remember how to call python from bash and include libraries&lt;/p&gt;

&lt;h2 id=&#34;conclusion:7f16ea4a76eb101e46f16364e2326e6b&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;To sample random lines from command line, default &lt;code&gt;shuf&lt;/code&gt; from core utils is probably the best choice. It is very easy to use and outperforms others in term of execution time.
However, everything depends on a task. For machine learning problems sampling is not a bottleneck and might not require fastest execution.&lt;/p&gt;

&lt;h2 id=&#34;appendix:7f16ea4a76eb101e46f16364e2326e6b&#34;&gt;Appendix&lt;/h2&gt;

&lt;p&gt;Gist with benchmark file:
&lt;script src=&#34;//gist.github.com/34836af4fa1d6c2a0dfa.js&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Static site generator for personal blog</title>
      <link>http://kirillpavlov.com/blog/2015/08/22/static-site-generator-for-personal-blog/</link>
      <pubDate>Sat, 22 Aug 2015 23:31:19 +0800</pubDate>
      
      <guid>http://kirillpavlov.com/blog/2015/08/22/static-site-generator-for-personal-blog/</guid>
      <description>&lt;p&gt;Many of us would like to have personal identity in the Internet, write blog, share pictures, code and discuss interesting topics.
For tech related articles social networks would not be enought and we should look for personal blog or website solution.
Here I would like to explain some ideas behing my choice &amp;ndash; &lt;a href=&#34;https://gohugo.io&#34;&gt;hugo&lt;/a&gt;, static site generator written in &lt;a href=&#34;https://golang.org/&#34;&gt;Go&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First of all, let us write down requirements for personal page:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Web site should work even if we dont have time to support it&lt;/li&gt;
&lt;li&gt;It should support custom domains&lt;/li&gt;
&lt;li&gt;In case of blog, we should be able to add articles easily and deployment should not be problem&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A while ago I had personal page, stored on virtual machine in some cloud service.
This requires me to pay for VM and causes deployment difficulties.
I realized, that solution should be different.&lt;/p&gt;

&lt;p&gt;WordPress offers good service, but price of custom domain makes use of it questionable.
My decision was to use static site generator and deploy it to GitHub, because it is free.
I did not want to customise site a lot from the beginning, content is more important at that period.
Moreover, additional tools, such as Google Analytics and Discus comes with framework.&lt;/p&gt;

&lt;p&gt;There are &lt;a href=&#34;https://www.staticgen.com/&#34;&gt;a lot of static site generators&lt;/a&gt; on the market.
How to choose right one?
My goal was to choose something simple, yet flexible to be able to use it in other projects as well.&lt;/p&gt;

&lt;p&gt;First of all I checked python Pelican, because of my language knowledge.
It looks very similar to Django.
I did not really find it interesting and it&amp;rsquo;s own website was a bit ugly.&lt;/p&gt;

&lt;p&gt;Next, I try JavaScript based generators.
From my point of view, technology itself should be as close to frontend development as possible.
For example, I would rather go for JavaScript instead of Ruby.
I was not able to setup Assemble in 20-30 minutes and found it not easy to work with.
Another JavaScript tools I try were Metalsmith and Hexo.
They require their custom plugins for everything and I don&amp;rsquo;t understand, why it is better than more generic plugins of Grunt or Gulp.&lt;/p&gt;

&lt;p&gt;Next candidate was Jekyll.
It has at least twice as more GitHub stars, than second popular solution.
It is also default GitHub pages solution.
Frankly speaking, Jekyll looks good, but a bit big, so it might be difficult to write own plugins.
At that point of time, I wanted to avoid Ruby and try something else.&lt;/p&gt;

&lt;p&gt;My final choice was Hugo.
It is program, written in Golang, which provides functionality to create, develop and build static website.
As advantages I would like to mention:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Blazing fast build time (under 0.1 sec)&lt;/li&gt;
&lt;li&gt;Tag support: it generates search result pages for every tag used&lt;/li&gt;
&lt;li&gt;It has not only blog support, user could create any page with any url&lt;/li&gt;
&lt;li&gt;Google Analytics, Discuss, Gravatar, Social integration come out of the box&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;During development I use &lt;em&gt;develop&lt;/em&gt; branch for source code.
Hugo builds statis pages in &lt;em&gt;public&lt;/em&gt; folder, which is pushed to master branch using git-subtree.
You could read about this technique &lt;a href=&#34;http://gohugo.io/tutorials/github-pages-blog/#configure-git-workflow&#34;&gt;here&lt;/a&gt;.
To simplify deployment, there is Makefile command with following code:&lt;/p&gt;

&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;hugo
git add -A
git commit -m &lt;span style=&#34;color: #e6db74&#34;&gt;&amp;quot;rebuilding site &amp;#39;&lt;/span&gt;&lt;span style=&#34;color: #66d9ef&#34;&gt;$(&lt;/span&gt;shell date&lt;span style=&#34;color: #66d9ef&#34;&gt;)&lt;/span&gt;&lt;span style=&#34;color: #e6db74&#34;&gt;&amp;#39;&amp;quot;&lt;/span&gt;
git push origin develop
git subtree push --prefix&lt;span style=&#34;color: #f92672&#34;&gt;=&lt;/span&gt;public git@github.com:pavlov99/pavlov99.github.com.git master
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I would like to recommend Hugo for anybody, who wants to build static pages.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>http://kirillpavlov.com/about/</link>
      <pubDate>Sun, 09 Aug 2015 16:20:37 +0800</pubDate>
      
      <guid>http://kirillpavlov.com/about/</guid>
      <description>

&lt;p&gt;Hi! My name is Kirill Pavlov, I&amp;rsquo;m a data scientist and software engeneer.
While I&amp;rsquo;m working on content update, check out my &lt;a href=&#34;http://kirillpavlov.com/cv/cv-kirill-pavlov.pdf&#34;&gt;CV&lt;/a&gt;,
&lt;a href=&#34;http://resume.github.io/?pavlov99&#34;&gt;github resume&lt;/a&gt; and &lt;a href=&#34;http://osrc.dfm.io/pavlov99&#34;&gt;github report card&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My PGP key is on &lt;a href=&#34;https://keybase.io/p99&#34;&gt;keybase&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am also doing project Euler tasks, you could track the progress here:
&lt;img src=&#34;https://projecteuler.net/profile/pavlov99.png&#34; alt=&#34;Project Euler badge&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;my-name:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;My name&lt;/h2&gt;

&lt;p&gt;My first name is &amp;ldquo;Kirill&amp;rdquo; and my last name is &amp;ldquo;Pavlov&amp;rdquo;.
This name &lt;a href=&#34;https://en.wikipedia.org/wiki/Kirill&#34;&gt;came from Greek&lt;/a&gt;.
One of the inventors of &lt;a href=&#34;https://en.wikipedia.org/wiki/Cyrillic_alphabets&#34;&gt;Cyrillic Alpabet&lt;/a&gt; is Saint Cyrill, our names are considered the same .&lt;/p&gt;

&lt;p&gt;In English my name is pronounced as [k&amp;rsquo;iriɛl p&amp;rsquo;avlov].
My Chinese name is 巴吉霖 (bā jí lín).&lt;/p&gt;

&lt;h2 id=&#34;contact-me:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;Contact me&lt;/h2&gt;

&lt;p&gt;Please use e-mail: pavlov99 [at] yandex.ru&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>